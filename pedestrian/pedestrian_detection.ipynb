{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wUwZ7GW9K2S"
      },
      "outputs": [],
      "source": [
        "# load dataset -- copying them from Google Drive\n",
        "!cp drive/MyDrive/coco.zip .\n",
        "!cp drive/MyDrive/PennFudanPed.zip .\n",
        "!cp drive/MyDrive/engine.py .\n",
        "!cp drive/MyDrive/coco_eval.py .\n",
        "!cp drive/MyDrive/coco_utils.py .\n",
        "!cp drive/MyDrive/transforms.py .\n",
        "!cp drive/MyDrive/utils.py .\n",
        "!unzip coco.zip\n",
        "!unzip PennFudanPed.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RNdL4Gto9Vj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_channel, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.cls_score = nn.Linear(input_channel, num_classes)\n",
        "        self.bbox_pred = nn.Linear(input_channel, num_classes * 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "      if x.dim() == 4:\n",
        "            assert list(x.shape[2:]) == [1, 1]\n",
        "      x = x.flatten(start_dim=1)\n",
        "      \n",
        "      scores = self.cls_score(x)\n",
        "      bbox_coord = self.bbox_pred(x)\n",
        "      return scores, bbox_coord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W79lIxtx7Rx3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from pycocotools.coco import COCO\n",
        "from PIL import Image\n",
        "import os\n",
        "import torchvision.transforms as T\n",
        "\n",
        "class CocoDataset(Dataset):\n",
        "    def __init__(self, annotation_file, images_file, show_bbox=False):\n",
        "        self.showbbox = show_bbox\n",
        "        self.coco = COCO(annotation_file)\n",
        "        self.images_file = images_file \n",
        "\n",
        "        # only person images\n",
        "        catIds = self.coco.getCatIds(catNms=['person'])\n",
        "        self.ids = list(sorted(self.coco.getImgIds(catIds=catIds)))\n",
        "        \n",
        "        # all images\n",
        "        # self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        id_image = self.ids[index]\n",
        "        \n",
        "        img_name = self.coco.loadImgs(id_image)[0][\"file_name\"]\n",
        "        img = Image.open(os.path.join(self.images_file, img_name))\n",
        "\n",
        "        annotations_id = self.coco.getAnnIds(imgIds=id_image)\n",
        "        annotations = self.coco.loadAnns(annotations_id)\n",
        "        \n",
        "        \n",
        "        num_objs = len(annotations)\n",
        "\n",
        "        boxes = []\n",
        "        areas = []\n",
        "        labels = []\n",
        "        for j in range(num_objs):\n",
        "          if annotations[j]['category_id'] == 1: # only person labels\n",
        "            x_min = annotations[j]['bbox'][0]\n",
        "            y_min = annotations[j]['bbox'][1]\n",
        "            x_max = x_min + annotations[j]['bbox'][2]\n",
        "            y_max = y_min + annotations[j]['bbox'][3]\n",
        "            boxes.append([x_min, y_min, x_max, y_max])\n",
        "            areas.append(annotations[j]['area'])\n",
        "            labels.append(annotations[j]['category_id'])\n",
        "\n",
        "        if num_objs == 0:\n",
        "          boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "        else:\n",
        "          boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.long)\n",
        "        id_image = torch.tensor([id_image])\n",
        "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "\n",
        "        Annotations = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"image_id\": id_image,\n",
        "            \"area\": areas,\n",
        "            \"iscrowd\": iscrowd\n",
        "        }\n",
        "    \n",
        "        tr=T.Compose([\n",
        "            T.ToTensor(),\n",
        "        ])\n",
        "        return tr(img), Annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfJoQoMu47Rt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "\n",
        "class PennFudanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path)\n",
        "        mask = np.array(mask)\n",
        "        obj_ids = np.unique(mask)\n",
        "        obj_ids = obj_ids[1:]\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "        target[\"img_path\"] = img_path\n",
        "\n",
        "        tr = T.ToTensor()\n",
        "        img = tr(img)\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfWgCNrjp5Py",
        "outputId": "6ef1d272-37fb-45d2-e961-34a6d50a9143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.57s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.75s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.70s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "annotation_file='coco/annotations/instances_val2017.json'\n",
        "images_file='coco/val2017'\n",
        "dataset = CocoDataset(annotation_file, images_file)\n",
        "dataset_validation = CocoDataset(annotation_file, images_file)\n",
        "dataset_test_coco = CocoDataset(annotation_file, images_file)\n",
        "\n",
        "dataset_test_pennfudan = PennFudanDataset('PennFudanPed')\n",
        "\n",
        "\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "indices_pennfudan = torch.randperm(len(dataset_test_pennfudan)).tolist()\n",
        "\n",
        "\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:100])\n",
        "dataset_validation = torch.utils.data.Subset(dataset_validation, indices[700:720])\n",
        "dataset_test_coco = torch.utils.data.Subset(dataset_test_coco, indices[500:520])\n",
        "dataset_test_pennfudan = torch.utils.data.Subset(dataset_test_pennfudan, indices_pennfudan[0:100])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=2,\n",
        "    collate_fn=collate_fn)\n",
        "\n",
        "data_loader_validation = torch.utils.data.DataLoader(\n",
        "    dataset_validation, batch_size=2, shuffle=True, num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "data_loader_test_coco = torch.utils.data.DataLoader(\n",
        "    dataset_test_coco, batch_size=1, shuffle=False, num_workers=2,\n",
        "    collate_fn=collate_fn)\n",
        "data_loader_test_pennfudan = torch.utils.data.DataLoader(\n",
        "    dataset_test_pennfudan, batch_size=1, shuffle=False, num_workers=2,\n",
        "    collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYKe4eczorVE"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "function for validation loss\n",
        "https://stackoverflow.com/questions/71288513/how-can-i-determine-validation-loss-for-faster-rcnn-pytorch\n",
        "'''\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from collections import OrderedDict\n",
        "from torchvision.models.detection.roi_heads import fastrcnn_loss\n",
        "from torchvision.models.detection.rpn import concat_box_prediction_layers\n",
        "def eval_forward(model, images, targets):\n",
        "    model.eval()\n",
        "\n",
        "    original_image_sizes: List[Tuple[int, int]] = []\n",
        "    for img in images:\n",
        "        val = img.shape[-2:]\n",
        "        assert len(val) == 2\n",
        "        original_image_sizes.append((val[0], val[1]))\n",
        "\n",
        "    images, targets = model.transform(images, targets)\n",
        "    if targets is not None:\n",
        "        for target_idx, target in enumerate(targets):\n",
        "            boxes = target[\"boxes\"]\n",
        "            degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n",
        "            if degenerate_boxes.any():\n",
        "                bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n",
        "                degen_bb: List[float] = boxes[bb_idx].tolist()\n",
        "                raise ValueError(\n",
        "                    \"All bounding boxes should have positive height and width.\"\n",
        "                    f\" Found invalid box {degen_bb} for target at index {target_idx}.\"\n",
        "                )\n",
        "\n",
        "    features = model.backbone(images.tensors)\n",
        "    if isinstance(features, torch.Tensor):\n",
        "        features = OrderedDict([(\"0\", features)])\n",
        "    model.rpn.training=True\n",
        "    features_rpn = list(features.values())\n",
        "    objectness, pred_bbox_deltas = model.rpn.head(features_rpn)\n",
        "    anchors = model.rpn.anchor_generator(images, features_rpn)\n",
        "\n",
        "    num_images = len(anchors)\n",
        "    num_anchors_per_level_shape_tensors = [o[0].shape for o in objectness]\n",
        "    num_anchors_per_level = [s[0] * s[1] * s[2] for s in num_anchors_per_level_shape_tensors]\n",
        "    objectness, pred_bbox_deltas = concat_box_prediction_layers(objectness, pred_bbox_deltas)\n",
        "    proposals = model.rpn.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n",
        "    proposals = proposals.view(num_images, -1, 4)\n",
        "    proposals, scores = model.rpn.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)\n",
        "\n",
        "    proposal_losses = {}\n",
        "    assert targets is not None\n",
        "    labels, matched_gt_boxes = model.rpn.assign_targets_to_anchors(anchors, targets)\n",
        "    regression_targets = model.rpn.box_coder.encode(matched_gt_boxes, anchors)\n",
        "    loss_objectness, loss_rpn_box_reg = model.rpn.compute_loss(\n",
        "        objectness, pred_bbox_deltas, labels, regression_targets\n",
        "    )\n",
        "    proposal_losses = {\n",
        "        \"loss_objectness\": loss_objectness,\n",
        "        \"loss_rpn_box_reg\": loss_rpn_box_reg,\n",
        "    }\n",
        "\n",
        "    image_shapes = images.image_sizes\n",
        "    proposals, matched_idxs, labels, regression_targets = model.roi_heads.select_training_samples(proposals, targets)\n",
        "    box_features = model.roi_heads.box_roi_pool(features, proposals, image_shapes)\n",
        "    box_features = model.roi_heads.box_head(box_features)\n",
        "    class_logits, box_regression = model.roi_heads.box_predictor(box_features)\n",
        "\n",
        "    result: List[Dict[str, torch.Tensor]] = []\n",
        "    detector_losses = {}\n",
        "    loss_classifier, loss_box_reg = fastrcnn_loss(class_logits, box_regression, labels, regression_targets)\n",
        "    detector_losses = {\"loss_classifier\": loss_classifier, \"loss_box_reg\": loss_box_reg}\n",
        "    boxes, scores, labels = model.roi_heads.postprocess_detections(class_logits, box_regression, proposals, image_shapes)\n",
        "    num_images = len(boxes)\n",
        "    for i in range(num_images):\n",
        "        result.append(\n",
        "            {\n",
        "                \"boxes\": boxes[i],\n",
        "                \"labels\": labels[i],\n",
        "                \"scores\": scores[i],\n",
        "            }\n",
        "        )\n",
        "    detections = result\n",
        "    detections = model.transform.postprocess(detections, images.image_sizes, original_image_sizes)\n",
        "    model.rpn.training=False\n",
        "    model.roi_heads.training=False\n",
        "    losses = {}\n",
        "    losses.update(detector_losses)\n",
        "    losses.update(proposal_losses)\n",
        "    return losses, detections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OH8WxtoRMeI_"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from engine import evaluate\n",
        "\n",
        "\n",
        "def get_model(num_classes):\n",
        "    trainable_backbone_layers = 5\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, pretrained_backbone=True, trainable_backbone_layers=5)\n",
        "    # model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(\n",
        "    #    pretrained=True, pretrained_backbone=True, trainable_backbone_layers=trainable_backbone_layers)\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = Classifier(in_features, num_classes)\n",
        "    \n",
        "    return model\n",
        "\n",
        "# define parameters\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# num_classes = 91 # all coco classes\n",
        "num_classes = 2 # person + background\n",
        "\n",
        "model = get_model(num_classes)\n",
        "model.to(device)\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "# optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
        "optimizer = torch.optim.Adam(params, lr=0.005, betas=(0.9, 0.999), eps=0.1, weight_decay=0)\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "\n",
        "# training starts here\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    i = 0\n",
        "    for images, targets in data_loader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        loss = sum(loss for loss in loss_dict.values())\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"iteration: {i}\\tloss: {loss}\")\n",
        "        i += 1\n",
        "\n",
        "    # validation loss\n",
        "    loss_validation_dict, dets = eval_forward(model, images, targets)\n",
        "    loss_validation = sum(loss for loss in loss_validation_dict.values())\n",
        "\n",
        "    print(f\"validadion_loss: {loss_validation}\\n\\n\")\n",
        "    print(f\"[test] evaluating coco\")\n",
        "    # evaluate(model, data_loader_test_coco, device)\n",
        "    print(f\"\\n[test] evaluating pennfudan\")\n",
        "    # evaluate(model, data_loader_test_pennfudan, device) \n",
        "    lr_scheduler.step()\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CJJFwESNDyS"
      },
      "outputs": [],
      "source": [
        "# save the model\n",
        "weights_path='drive/MyDrive/5-performance-loss--model_resnet50-lr_0_005-epoch_10-optim_pth.pth'\n",
        "torch.save({\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': loss},\n",
        "     weights_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viEL1m1tpgKN"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow # used for plotting images on Colab\n",
        "\n",
        "tr = T.ToTensor()\n",
        "\n",
        "model.eval()\n",
        "i = 0\n",
        "for images, targets in data_loader_test_pennfudan:\n",
        "  images = list(img.to(device) for img in images)\n",
        "  \n",
        "  if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "  predictions = model(images)\n",
        "\n",
        "  # coco\n",
        "  # img_name = '0' * (12 - len(str(targets[0]['image_id'].item()))) + str(targets[0]['image_id'].item()) + '.jpg'\n",
        "  # img = cv2.imread('coco/val2017/' + img_name)\n",
        "\n",
        "  # penndufudan\n",
        "  img_name = targets[0]['img_path']\n",
        "  print(img_name)\n",
        "  img = cv2.imread(img_name)\n",
        "\n",
        "  for prediction in predictions:\n",
        "    boxes = prediction['boxes']\n",
        "    labels = prediction['labels']\n",
        "    scores = prediction['scores']\n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "      if score > 0.8:\n",
        "        print(box, label, score)\n",
        "        cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)\n",
        "\n",
        "    cv2_imshow(img)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "pedestrian detection - coco dataset",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}