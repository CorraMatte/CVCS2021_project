{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RNdL4Gto9Vj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_channel, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        # Conf 1\n",
        "        self.classification = nn.Sequential(\n",
        "            nn.Linear(input_channel, 50),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 50),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(50, 50),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, num_classes)\n",
        "        )\n",
        "        \n",
        "        # regression layers (for bounding boxes)\n",
        "        self.regression = nn.Sequential(\n",
        "            nn.Linear(input_channel, 50),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 50),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, num_classes * 4)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.flatten(start_dim=1)\n",
        "        scores = self.classification(x)\n",
        "        bbox_coord = self.regression(x)\n",
        "        return scores, bbox_coord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W79lIxtx7Rx3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from pycocotools.coco import COCO\n",
        "from PIL import Image\n",
        "import os\n",
        "import torchvision.transforms as T\n",
        "\n",
        "class CocoDataset(Dataset):\n",
        "    def __init__(self, annotation_file, images_file, show_bbox=False):\n",
        "        self.showbbox = show_bbox\n",
        "        self.coco = COCO(annotation_file)\n",
        "        self.images_file = images_file \n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        id_image = self.ids[index]\n",
        "        \n",
        "        img_name = self.coco.loadImgs(id_image)[0][\"file_name\"]\n",
        "        img = Image.open(os.path.join(self.images_file, img_name))\n",
        "\n",
        "        annotations_id = self.coco.getAnnIds(imgIds=id_image)\n",
        "        annotations = self.coco.loadAnns(annotations_id)\n",
        "        \n",
        "        \n",
        "        num_objs = len(annotations)\n",
        "\n",
        "        boxes = []\n",
        "        areas = []\n",
        "        labels = []\n",
        "        for j in range(num_objs):\n",
        "            x_min = annotations[j]['bbox'][0]\n",
        "            y_min = annotations[j]['bbox'][1]\n",
        "            x_max = x_min + annotations[j]['bbox'][2]\n",
        "            y_max = y_min + annotations[j]['bbox'][3]\n",
        "            boxes.append([x_min, y_min, x_max, y_max])\n",
        "            areas.append(annotations[j]['area'])\n",
        "            labels.append(annotations[j]['category_id'])\n",
        "\n",
        "        if num_objs == 0:\n",
        "          boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "        else:\n",
        "          boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.long)\n",
        "        id_image = torch.tensor([id_image])\n",
        "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "\n",
        "        Annotations = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels,\n",
        "            \"image_id\": id_image,\n",
        "            \"area\": areas,\n",
        "            \"iscrowd\": iscrowd\n",
        "        }\n",
        "\n",
        "        return T.ToTensor()(img), Annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfWgCNrjp5Py",
        "outputId": "99aee914-05a4-44b2-dc1e-a35182ea1579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.66s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.83s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.74s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "annotation_file='drive/MyDrive/coco/annotations/instances_val2017.json'\n",
        "images_file='drive/MyDrive/coco/val2017'\n",
        "dataset = CocoDataset(annotation_file, images_file)\n",
        "dataset_validation = CocoDataset(annotation_file, images_file)\n",
        "dataset_test = CocoDataset(annotation_file, images_file)\n",
        "\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:300])\n",
        "dataset_validation = torch.utils.data.Subset(dataset_validation, indices[500:600])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[600:700])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=2,\n",
        "    collate_fn=collate_fn)\n",
        "\n",
        "data_loader_validation = torch.utils.data.DataLoader(\n",
        "    dataset_validation, batch_size=2, shuffle=True, num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=2,\n",
        "    collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7qu9_DZKFKu"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "def get_model(num_classes):\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = Classifier(in_features, num_classes)\n",
        "    # model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "6191577d5b744dad888a315c59ac2e17",
            "24a9ef8484884a8a8cb06a737f4dbf35",
            "b3d5cf39e9974352946faa3358200f45",
            "6ca4d21fb96a4a46ac67eebebb324f0c",
            "ccad9bd8c001408c929e6589270b0b12",
            "a417c05891ba4750b267c06bf63cd2d8",
            "65920df23d8b461e8447a4348a476959",
            "c534fd4c8009479bb718434dfd06ef89",
            "8b38379692534d09beb4ab2a6ad81108",
            "df3d3fe3b9ee48bf9bf203236f06f246",
            "e6b24642bb5b49b5932ea439097cfebc"
          ]
        },
        "id": "JW6-mUPRLONO",
        "outputId": "19c4146e-2447-4b69-b03f-67bd611db98b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6191577d5b744dad888a315c59ac2e17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/160M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "num_classes = 91\n",
        "\n",
        "model = get_model(num_classes)\n",
        "model.to(device)\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "# optimizer = torch.optim.ADAM(params, lr=0.005,\n",
        "#                            momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYKe4eczorVE"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "function for validation loss\n",
        "https://stackoverflow.com/questions/71288513/how-can-i-determine-validation-loss-for-faster-rcnn-pytorch\n",
        "'''\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from collections import OrderedDict\n",
        "from torchvision.models.detection.roi_heads import fastrcnn_loss\n",
        "from torchvision.models.detection.rpn import concat_box_prediction_layers\n",
        "def eval_forward(model, images, targets):\n",
        "    model.eval()\n",
        "\n",
        "    original_image_sizes: List[Tuple[int, int]] = []\n",
        "    for img in images:\n",
        "        val = img.shape[-2:]\n",
        "        assert len(val) == 2\n",
        "        original_image_sizes.append((val[0], val[1]))\n",
        "\n",
        "    images, targets = model.transform(images, targets)\n",
        "    if targets is not None:\n",
        "        for target_idx, target in enumerate(targets):\n",
        "            boxes = target[\"boxes\"]\n",
        "            degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n",
        "            if degenerate_boxes.any():\n",
        "                # print the first degenerate box\n",
        "                bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n",
        "                degen_bb: List[float] = boxes[bb_idx].tolist()\n",
        "                raise ValueError(\n",
        "                    \"All bounding boxes should have positive height and width.\"\n",
        "                    f\" Found invalid box {degen_bb} for target at index {target_idx}.\"\n",
        "                )\n",
        "\n",
        "    features = model.backbone(images.tensors)\n",
        "    if isinstance(features, torch.Tensor):\n",
        "        features = OrderedDict([(\"0\", features)])\n",
        "    model.rpn.training=True\n",
        "    #####proposals, proposal_losses = model.rpn(images, features, targets)\n",
        "    features_rpn = list(features.values())\n",
        "    objectness, pred_bbox_deltas = model.rpn.head(features_rpn)\n",
        "    anchors = model.rpn.anchor_generator(images, features_rpn)\n",
        "\n",
        "    num_images = len(anchors)\n",
        "    num_anchors_per_level_shape_tensors = [o[0].shape for o in objectness]\n",
        "    num_anchors_per_level = [s[0] * s[1] * s[2] for s in num_anchors_per_level_shape_tensors]\n",
        "    objectness, pred_bbox_deltas = concat_box_prediction_layers(objectness, pred_bbox_deltas)\n",
        "    # apply pred_bbox_deltas to anchors to obtain the decoded proposals\n",
        "    # note that we detach the deltas because Faster R-CNN do not backprop through\n",
        "    # the proposals\n",
        "    proposals = model.rpn.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n",
        "    proposals = proposals.view(num_images, -1, 4)\n",
        "    proposals, scores = model.rpn.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)\n",
        "\n",
        "    proposal_losses = {}\n",
        "    assert targets is not None\n",
        "    labels, matched_gt_boxes = model.rpn.assign_targets_to_anchors(anchors, targets)\n",
        "    regression_targets = model.rpn.box_coder.encode(matched_gt_boxes, anchors)\n",
        "    loss_objectness, loss_rpn_box_reg = model.rpn.compute_loss(\n",
        "        objectness, pred_bbox_deltas, labels, regression_targets\n",
        "    )\n",
        "    proposal_losses = {\n",
        "        \"loss_objectness\": loss_objectness,\n",
        "        \"loss_rpn_box_reg\": loss_rpn_box_reg,\n",
        "    }\n",
        "\n",
        "    #####detections, detector_losses = model.roi_heads(features, proposals, images.image_sizes, targets)\n",
        "    image_shapes = images.image_sizes\n",
        "    proposals, matched_idxs, labels, regression_targets = model.roi_heads.select_training_samples(proposals, targets)\n",
        "    box_features = model.roi_heads.box_roi_pool(features, proposals, image_shapes)\n",
        "    box_features = model.roi_heads.box_head(box_features)\n",
        "    class_logits, box_regression = model.roi_heads.box_predictor(box_features)\n",
        "\n",
        "    result: List[Dict[str, torch.Tensor]] = []\n",
        "    detector_losses = {}\n",
        "    loss_classifier, loss_box_reg = fastrcnn_loss(class_logits, box_regression, labels, regression_targets)\n",
        "    detector_losses = {\"loss_classifier\": loss_classifier, \"loss_box_reg\": loss_box_reg}\n",
        "\n",
        "    return detector_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OH8WxtoRMeI_"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    i = 0\n",
        "    for images, targets in data_loader:\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # during training the model's output is a dictionary with losses        \n",
        "        loss_dict = model(images, targets)\n",
        "        # print(loss_dict)\n",
        "        # loss = sum(loss for loss in loss_dict.values())\n",
        "        loss = loss_dict['loss_classifier'] + loss_dict['loss_box_reg']\n",
        "        # print(f\"iteration: {i}\\tloss: {loss}\")\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # if i % 10 == 0:\n",
        "        print(f\"iteration: {i}\\tloss: {loss}\")\n",
        "        i += 1\n",
        "\n",
        "    # validation loss\n",
        "    loss_validation_dict = eval_forward(model, images, targets)\n",
        "    loss_validation = loss_validation_dict['loss_classifier'] + loss_validation_dict['loss_box_reg']\n",
        "    print(f\"validadion_loss: {loss_validation}\")\n",
        "    lr_scheduler.step()\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CJJFwESNDyS"
      },
      "outputs": [],
      "source": [
        "weights_name='weights_head-fastrcnnpredictor_backbone-resnet50_dataset-coco.pth'\n",
        "torch.save(model.state_dict(), weights_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82lMG4ctG6mf"
      },
      "outputs": [],
      "source": [
        "# performance\n",
        "import cv2\n",
        "import json\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "\n",
        "# load model\n",
        "# model = get_model(91)\n",
        "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# cpu\n",
        "# model.load_state_dict(torch.load(\n",
        "#     'drive/MyDrive/weights_head-fastrcnnpredictor_backbone-resnet50_dataset-coco.pth',\n",
        "#      map_location=torch.device('cpu')))\n",
        "\n",
        "# gpu\n",
        "# model.load_state_dict(torch.load(\n",
        "#    '/content/drive/MyDrive/weights_head-fastrcnnpredictor_backbone-resnet50_dataset-coco.pth'))\n",
        "\n",
        "tr = T.ToTensor()\n",
        "results = []\n",
        "coco_format_dict ={\n",
        "    \"image_id\": 0,\n",
        "    \"category_id\": 0,\n",
        "    \"bbox\":[0, 0, 0, 0],\n",
        "    \"score\": 0\n",
        "}\n",
        "i = 0\n",
        "n_iter = 0\n",
        "model.eval()\n",
        "with open(\"results.json\", \"w\") as outfile:\n",
        "  outfile.write('[')\n",
        "  for images, targets in data_loader_test:\n",
        "    # print(f\"# iter: {n_iter}\")\n",
        "    images = list(img.to(device) for img in images)\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "              torch.cuda.synchronize()\n",
        "\n",
        "    predictions = model(images)\n",
        "\n",
        "    for prediction, target in zip(predictions, targets):\n",
        "      boxes = prediction[\"boxes\"]\n",
        "      labels = prediction[\"labels\"]\n",
        "      scores = prediction[\"scores\"]\n",
        "\n",
        "      coco_format_dict ={\n",
        "          \"image_id\": target[\"image_id\"].item(),\n",
        "          \"category_id\": 0,\n",
        "          \"bbox\":[0, 0, 0, 0],\n",
        "          \"score\": 0\n",
        "      }\n",
        "\n",
        "      for box, label, score in zip(boxes, labels, scores):\n",
        "        coco_format_dict[\"category_id\"] = label.item()\n",
        "        if score > 0.7:\n",
        "          # model box format         [x_min, y_min, x_max, y_max]\n",
        "          # coco box format          [x_min,  y_min,  width=x_max-x_min, height=y_max-y_min]\n",
        "          coco_format_dict[\"bbox\"] = [box[0].item(), box[1].item(), box[2].item() - box[0].item(),   box[3].item() - box[1].item()]\n",
        "          coco_format_dict[\"score\"] = score.item()\n",
        "          # print(json.dumps(coco_format_dict))\n",
        "          if i > 0:\n",
        "            outfile.write(',')\n",
        "          i += 1\n",
        "          outfile.write(json.dumps(coco_format_dict))\n",
        "    n_iter += 1\n",
        "  outfile.write(']')\n",
        "\n",
        "dts = json.load(open(\"results.json\", 'r'))\n",
        "imgIds = [imid['image_id'] for imid in dts]\n",
        "imgIds = sorted(list(set(imgIds)))\n",
        "del dts\n",
        "\n",
        "cocoGt = COCO('drive/MyDrive/coco/annotations/instances_val2017.json')\n",
        "cocoDt = cocoGt.loadRes('results.json')\n",
        "cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
        "cocoEval.params.imgIds = imgIds\n",
        "# cocoEval.params.catIds = [1]\n",
        "cocoEval.evaluate()\n",
        "cocoEval.accumulate()\n",
        "cocoEval.summarize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "viEL1m1tpgKN"
      },
      "outputs": [],
      "source": [
        "# evaluation\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# model = get_model(91)\n",
        "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# cpu\n",
        "# model.load_state_dict(torch.load(\n",
        "#     'drive/MyDrive/weights_head-fastrcnnpredictor_backbone-resnet50_dataset-coco.pth',\n",
        "#      map_location=torch.device('cpu')))\n",
        "\n",
        "# gpu\n",
        "# model_test.load_state_dict(torch.load(\n",
        "#    '/content/drive/MyDrive/weights_head-fastrcnnpredictor_backbone-resnet50_dataset-coco.pth'))\n",
        "\n",
        "tr = T.ToTensor()\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for images, targets in data_loader_test:\n",
        "  images = list(img.to(device) for img in images)\n",
        "  \n",
        "  if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "  predictions = model(images)\n",
        "\n",
        "  img_name = '0' * (12 - len(str(targets[0]['image_id'].item()))) + str(targets[0]['image_id'].item()) + '.jpg'\n",
        "  img = cv2.imread('drive/MyDrive/coco/val2017/' + img_name)\n",
        "\n",
        "  for prediction in predictions:\n",
        "    boxes = prediction['boxes']\n",
        "    labels = prediction['labels']\n",
        "    scores = prediction['scores']\n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "      if score > 0.5:\n",
        "        print(box, label, score)\n",
        "        cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)\n",
        "\n",
        "    cv2_imshow(img)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "pedestrian detection - coco dataset",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "24a9ef8484884a8a8cb06a737f4dbf35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a417c05891ba4750b267c06bf63cd2d8",
            "placeholder": "​",
            "style": "IPY_MODEL_65920df23d8b461e8447a4348a476959",
            "value": "100%"
          }
        },
        "6191577d5b744dad888a315c59ac2e17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24a9ef8484884a8a8cb06a737f4dbf35",
              "IPY_MODEL_b3d5cf39e9974352946faa3358200f45",
              "IPY_MODEL_6ca4d21fb96a4a46ac67eebebb324f0c"
            ],
            "layout": "IPY_MODEL_ccad9bd8c001408c929e6589270b0b12"
          }
        },
        "65920df23d8b461e8447a4348a476959": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ca4d21fb96a4a46ac67eebebb324f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df3d3fe3b9ee48bf9bf203236f06f246",
            "placeholder": "​",
            "style": "IPY_MODEL_e6b24642bb5b49b5932ea439097cfebc",
            "value": " 160M/160M [00:01&lt;00:00, 136MB/s]"
          }
        },
        "8b38379692534d09beb4ab2a6ad81108": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a417c05891ba4750b267c06bf63cd2d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3d5cf39e9974352946faa3358200f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c534fd4c8009479bb718434dfd06ef89",
            "max": 167502836,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b38379692534d09beb4ab2a6ad81108",
            "value": 167502836
          }
        },
        "c534fd4c8009479bb718434dfd06ef89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccad9bd8c001408c929e6589270b0b12": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df3d3fe3b9ee48bf9bf203236f06f246": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6b24642bb5b49b5932ea439097cfebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}